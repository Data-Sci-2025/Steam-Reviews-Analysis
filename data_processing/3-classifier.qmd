---
title: "3-classifier"
format: gfm
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(caret)
library(mlbench)
library(klaR)
library(ranger)
```

**Note**: Please run [0-data-exploration](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/0-data-exploration.qmd), [1-data-cleanup](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/1-data-cleanup.qmd), and [2-data-analysis](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/2-data-analysis.qmd) first to create the version of the .csv file needed to start here.

## Classifier Data


To get another look at the df to be used to train the classifier later. This is the tf-idf information for the downsampled version of the game reviews data, stop words removed. 


```{r}
#| label: readingin
review_tf_idf <- read_csv("../private/reviews_tfidf.csv", show_col_types = FALSE)

review_tf_idf
```



### Remove NA 

Just to double check and clean out any troublesome NA values:


```{r}
#| label: findNA
colSums(is.na(review_tf_idf))
```

```{r}
#| label: rmNA
review_tf_idf <- na.omit(review_tf_idf)
```

```{r}
#| label: confirmNA
colSums(is.na(review_tf_idf))
```


```{r}
#| label: revcount
review_tf_idf |>
  group_by(review_type) |>
  summarise(count=n())
```


## One more cleanup

As was covered toward the end of my analysis notebook, there are many many words that are the same between both review types (the classes our classifier is going to attempt to predict). Because these words appear so commonly in both classes, it gave the classifier a tough time trying to predict a review's class. My first attempt guessed every single review negative. 

After talking with Dan we decided the best move would be to make a custom list of stop words using these highly common and overlapping words (that will have a really low tf-idf value anyway) to break it down into the more distinctive words and maybe get a cleaner prediction.


```{r}
#| label: revsplit
posfreq <- review_tf_idf |>
  filter(review_type=='POS') |>
  arrange(tf_idf)
posfreq

negfreq <- review_tf_idf |>
  filter(review_type=='NEG') |>
  arrange(tf_idf)
negfreq
```


```{r}
#| label: tf-idf-sum
summary(review_tf_idf$tf_idf)
```

```{r}
#| label: filter-revs
reviews_shortened <- review_tf_idf |>
  filter(tf_idf > .009)

reviews_shortened |>
  group_by(review_type) |>
  summarise(count=n())
```



```{r}
#| label: stopwords
mystopwords <- tibble(word = c("game", "10", "2", "3", "play", "1", "4", "5", "time", "fun", "story", "games", "buy", "20", "gameplay", "6", "worth", "times", "100", "playing", "played", "30", "world", "9", "recommend", "8", "7"))
```


By looking through the first several rows of the dataframe above as well as the top tokens shared that I spotted in my analysis notebook, I've added a list of highly frequent and shared words that wouldn't indicate either POS or NEG review strongly in either direction. 


```{r}
#| label: join-stopwords
reviews_shortened <- anti_join(review_tf_idf, mystopwords, by="word")
```


```{r}
#| label: new-revcount
reviews_shortened |>
  group_by(review_type) |>
  summarise(count=n())
```

## Data Setup

Even after downsampling and removing stop words, I ran into issues trying to pivot my very large dataframe wider. 

Initially I sampled lines 1:400,000 of my data which worked well enough! But when I started to train the classifier on it, ran into an issue where even in all those rows, only POS marked reviews were appearing. 

Instead, I did a random sampling method of 300,000 rows of data and confirmed afterward that it contained both NEG and POS marked reviews. 

setup_sample <- sample_n(reviews_shortened, 800)

**Note that for rendered .md sampled slice is much smaller than in the .qmd file because rendering kept timing out**

```{r}
#| label: sample-slice
set.seed(14)
setup_sample <- reviews_shortened |>
  group_by(review_type) |>
  slice_sample(n = 150000)

setup_sample <-setup_sample |>
  rename(WC=total)
  
setup_sample
```

```{r}
#| label: setup-size
setup_sample |>
  group_by(review_type) |>
  summarise(count=n())
```



Columns (words) with digits as the column name were creating an issue 

```{r}
#| label: amend-cols
setup_sample <- setup_sample |>
  mutate(word = str_replace_all(word, "(^\\d.*$)", "i_\\1"))
```



Pivot wider so that all the individual words appear as column names, populated below by tf-idf values per review. 

```{r}
#| label: pivot-sample
setup_sample <- setup_sample |>
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0)

ncol(setup_sample)
```

Because of how the classifier works, any word that only appears once in any of the reviews (has only one tf-idf value over 0) would be discarded. If we train a classifier on the word "refund" and that word only appears once in this data subset, it would never encounter the word "refund" in the testing data to apply that classification weight to. It's wasted computing space and power on a word that it will never see again. 

To get rid of them in advance and ease the classification training process, this chunk will discard any column (word) that only has one value greater than 0 - any word that only appears a single time.


```{r}
#| label: remove-col
setup_sample <- setup_sample |>
  dplyr::select(!review_id)
```


The classifier needs this column to be factor type in order to run

```{r}
#| label: col-asfactor
setup_sample$review_type <- as.factor(setup_sample$review_type)
```


Remove any columns that have only one tf-idf value. It would indicate that they only show up once in the dataframe. If it makes it into the training split, it wouldn't appear in the testing data and would be a waste of training space. If it only appears in the testing data, it won't have been trained on it. A data point that only appears once is not a useful one. 

```{r}
#| label: remove-singles
setup_sample <- setup_sample |>
  purrr::discard(function(x) sum(x != 0) == 1)

ncol(setup_sample)
```



```{r}
#| label: setup-sample
head(setup_sample)
```


One last step to help the classifier run and train properly... some small elements like apostrophes were causing issues.

```{r}
#| label: cols-cleanup
setup_sample <- janitor::clean_names(setup_sample)
```


## Building a Classifier

Split the data by review type into training and testing data, with 75% of the data going to training.


```{r}
#| label: datasplitter
# split into training and testing sets

inTrain <- createDataPartition(
  y = setup_sample$review_type,
  ## the outcome data are needed
  p = .75,
  ## The percentage of data in the
  ## training set
  list = FALSE
)


```


```{r}
#| label: datasplit
training <- setup_sample[ inTrain,]
testing  <- setup_sample[-inTrain,]

nrow(training)
ncol(training)

nrow(testing)
ncol(testing)
```

```{r}
#| label: traincount
training |>
  group_by(review_type) |>
  summarise(count=n())
```



The classifier will be trained on 41,000 reviews and tested on close to 14,000 to see how well it can, by words used in a review, predict if a review itself is a positive or negative one.


## Training

I initially attempted to train on a larger subset of 400,000 samples, but the training ran for 6+ hours without end and I decided to downsize to 300,000. This training I ran overnight, so I don't know how long it actually took.


```{r}
#| label: training
rf_model <- ranger(dependent.variable.name = "review_type", 
                      data = training, 
                      num.trees = 40, 
                      mtry = 3,
                      min.node.size = 3,
                      importance = "permutation", 
                      seed = 123,
                      respect.unordered.factors = "ignore")
```



## Testing


```{r}
#| label: testing
p.ra <- predict(rf_model, data=testing)
str(p.ra)
```


Here are the results for this classifier when I included word count as a predicting variable. I had hope that, knowing the negative reviews are longer on average, including it as a factor would help the classifier be a little bit better at predicting the class of the review (POS or NEG). However, it seems to have only helped marginally. 

Despite all efforts to give my classifier the best chances at being informatively trained and able to classify reviews, it still wants to predict the Negative class for almost all reviews it's fed to test on. 


```{r}
#| label: confusion
confusion_matrix <- rf_model$confusion.matrix
    print(confusion_matrix)
```

Here are the results for this classifier when I excluded word count as a variable:

![](../notes_and_info/Classifier.PNG)








