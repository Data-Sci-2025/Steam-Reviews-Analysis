---
title: "2-data-analysis"
format: gfm
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(tidytext)
library(ggplot2)
library(highcharter)
library(palmerpenguins)
library(tidyr)
library(scales)
library(ggplot2)
library(forcats)
```

**Note**: Please run [0-data-exploration](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/0-data-exploration.qmd) and [1-data-cleanup](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/1-data-cleanup.qmd) first to create the version of the .csv file needed to start here.



```{r}
#| label: readingin
reviews_df <- read_csv("../private/reviews_analyze.csv", show_col_types = FALSE)
```


```{r}
#| label: toks-col
reviews_df <- reviews_df |>
  rename(tokens = merged_text)

reviews_df
```


## Goal Tracker

**List of Goals**

- some stats (correlation) - length and review type, review and rating type, maybe some others
- TF-IDF
- maybe let's look at some emoji usage (STARTED)

**Later Goals**

- build a classifier 


## Questions

1. Should I cap number of reviews per game to even things out?

```{r}
#| label: game-revs-c
reviews_df |>
  group_by(steam_id) |>
  summarise(count = n())
```

Some of these have way more reviews than others (positive games) and it could be affecting some of what I'm looking at objectively... on the other hand it's the most accurate look at the data. 

1a. If I do cut down on the number of reviews, should it be across the board for analysis, or just for some of the steps? 

2. I made some histograms for word length per review type (pos or neg) and they're both really skewed with a huge chunk of the reviews being very short (see the Length Stats tab here). It's making it so I can't really do a box plot or a violin plot because of the shape of things. Would a log transformation help? 

3. What about measuring correlation/the relationship between a numerical value and categorical? word count and review type. 

4. I've been looking at a lot of just word count and review type, what else should I be really looking at though...

5. This about exploring TTR and what to do with that. Visualizations?

6. Also I mentioned wanting to look at review flow and date. I want to see when reviews come in, when they taper off, if there are any spikes over time...

7. I don't know WHAT to do with these stupid emojis not being split up properly and I don't know if it matters or if I want to kind of start over on those entirely. Maybe if I do grepl using just the string of emoji unicodes and wipe out anything not in there. I might just create a separate emoji processing qmd and see what I can wrestle out there, make it a kind of extra goal.

7. similar toks charts not working why?????


## Some Missed Cleanup

While I was working on word counts I discovered a number of unicode blank spaces that were appearing as empty cells in the "review" column and were being counted as 1 word reviews. They were also messing with some of my later work with tokenization and tf-idf. 

I had to go through, find the blank rows among the 1 word reviews, use charToRaw on the review ID associated, and search online to find the unicode string to search for and remove.

I went over the top to remove from both reviews and token columns, just to be sure I was getting everything and removing the full row. It seemed like when I didn't, the rows weren't being entirely eliminated when they needed to be. Overkill is fine with me if the end result is what I'm after!


```{r}
#| label: remove-unicode
#| include: FALSE
reviews_df <- reviews_df |>
  mutate(review = str_replace_all(review, "\\\u200B+|\\\u200E+|\\\u200C+|\\\UE0021+|\\\u00AD+|\\\u200F+|^\\\u3164+$|\\\u180C|^\\\u0020+$|\\\u2063|\\\u2060|\\\u0009|\\\u00A0|\\\u034F|\\\u1160|\\\u2000|\\\u2003|\\\u2004|\\\u200D|\\\u202A|\\\u202C|\\\u202F|\\\u2067|\\\u3000|\\\u3164|\\\uFE0E|\\\uFE0F|\\\uFEFF", " ")) |>
  mutate(tokens = str_replace_all(tokens, "\\\u200B+|\\\u200E+|\\\u200C+|\\\UE0021+|\\\u00AD+|\\\u200F+|^\\\u3164+$|\\\u180C|^\\\u0020+$|\\\u2063|\\\u2060|\\\u0009|\\\u00A0|\\\u034F|\\\u1160|\\\u2000|\\\u2003|\\\u2004|\\\u200D|\\\u202A|\\\u202C|\\\u202F|\\\u2067|\\\u3000|\\\u3164|\\\uFE0E|\\\uFE0F|\\\uFEFF", " ")) |>
  mutate(review = str_replace_all(review, "  +", " ")) |>
  mutate(tokens = str_replace_all(tokens, "  +", " ")) |>
  mutate(review = review |>
      na_if(" ")) |>
  drop_na('review')
```


## Getting a Look at the Data

I'm going to move some of the basic data analysis in here later but I need it where it is for class 11/18


### Reviews over time

```{r}
#| eval: false
ggplot(dat, aes(x=Date_Published)) +   stat_count(geom='line', aes(y=..count..))
```



## Word Count

```{r}
#| label: word-count
reviews_df <- reviews_df |>
  mutate(word_count = str_count(tokens, '\\,')+1)

reviews_df
```

### Review length

First, there are a number of reviews that are just a blank space that were counted as having 1 word because of how I formulated the word count column. I'm going to declare these columns NA. If there's no text in the review, there's nothing there for me to analyze anyway. 


```{r}
#| label: WC-stats
summary(reviews_df$word_count)
```

So the average review is 51 words, and the median 17 words. 

Let's look a little deeper. 


```{r}
#| label: WC-low-high
reviews_df |>
  filter(word_count==1)

reviews_df |>
  filter(word_count>1700)
```

There's quite a few one-word reviews! Some of them are pretty reasonable... "Amazing", "Spooky", "Boring", "Unplayable". Short and sweet, gets the point across well enough. Others are a little less obvious. I saw a number of keyboard smash reviews, strings of numbers, I saw one that was just a rabbit emoji. The last one might be a reference I just don't understand.

Looking instead at these longest reviews, I was really surprised! A huge majority of these are exactly the same phrase repeated over and over, and those repeated reviews are all for the same game, as well. I have to do some investigation... 

Checking my game info [here](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/notes_and_info/0-gameinfo.csv), I was able to confirm that these repeating reviews are all from "The Stanley Parable: Ultra Deluxe". 

Looking further, I found some more information. ["The end is never"](https://thestanleyparable.fandom.com/wiki/The_End_Is_Never...) is a tagline for the game itself, and appears within the game multiple times as a reference to the inescapable time loop the game's protagonist is stuck in. The phrase also apparently appears on the game's loading screens in a constant loop. 


### Positive and Negative review length

First let's get a look at what we're working with. How many reviews of each type are there?

```{r}
#| label: plot-reviews
ggplot(reviews_df, aes(x=review_type, fill=review_type )) + 
  geom_bar( width = 0.5) +
  scale_fill_brewer(palette = "Set2") +
  geom_text(stat = "count", aes(label = after_stat(count)), size = 3.5, vjust = 3, hjust = 0.5, position = "stack") +
  theme(legend.position="right")
```

There are a LOT more positive reviews than negative ones. This made me curious, what about the total data. It's possible positively reviewed games have more reviews in general than negatively reviewed games? 

```{r}
#| label: games-info
games <- read_csv("../notes_and_info/0-gameinfo.csv")

games |>
  group_by(rank) |>
  #merging all varieties of pos/neg together by removing the first word per rank
  #very positive and overwhelmingly positive both just become positive etc
  mutate(rank = str_replace_all(rank, "\\w+ (\\w)", "\\1")) |>
  summarise(eng_reviews = sum(eng_reviews))
```

**Note** this equals 732,317 total reviews because I pulled the numbers straight from Steam game listings and not from the sheets I downloaded. At the time I wasn't thinking about it, but the script used to pull reviews would time out after a certain point and there was no guarantee I was getting the full number of reviews shown on Steam. Actually, it was very likely that for some of the more popular games there's almost no way I was getting all of them. 

Let's look at the accurate number.

```{r}
merged_games <- reviews_df |>
  left_join(games)

merged_games |>
  group_by(rank) |>
  #merging all varieties of pos/neg together by removing the first word per rank
  #very positive and overwhelmingly positive both just become positive etc
  mutate(rank = str_replace_all(rank, "\\w+ (\\w)", "\\1")) |>
  summarise(count = n())

```

= 189298, that's the same number of rows as reviews_df, so that adds up a little better.


It looks like kind of yes! 

My theory here just comes down to popularity. As a game releases and players try it out and review it, it either begins to migrate through the ranks of positive or negative. As new players find out about the game, if it shows already that it's being negatively reviewed, why would they spend money to try it out themselves? I wouldn't! On the other hand, if players see that a game is getting positive reviews, they'll more likely try it and, in turn, also positively review it.

Some of the games in the higher ranks in this data are hugely beloved games, while the negative ones are very likely largely forgotten to time once they descended into the negative review rankings. 

So, there it is. Positively ranked games have overall more reviews than negatively ranked games, and there are more positive reviews than negative ones in my data as a result. We'll keep that in mind, moving forward.


### Average Review Length

Now to what we came here to look into. Are positive reviews typically longer, or negative reviews? 


```{r}
#| label: plot-avg-len
ggplot(reviews_df, aes(x=review_type, y = word_count)) + 
  stat_summary(fun = mean, geom = "bar", fill = "skyblue") +
  geom_text(aes(label = after_stat(sprintf("%.2f", y))), stat = "summary", fun = "mean", vjust = 3, hjust=0.5) +
  ylab("Avg Amount") +
  theme(legend.position="right")
```

Now that's really interesting! Despite having fewer reviews total, and *all* of the longest reviews we looked at above being positive, negative reviews are still quite a bit longer on average. People must have a lot more to say when they dislike a game than when they like one!

I wonder if there's a trend in shorter reviews that might be swinging this average one way or the other. 

```{r}
#| label: rev-type-count
reviews_df |>
  group_by(review_type) |>
  filter(word_count<6) |>
  summarise(word_count = sum(word_count))
```

Looks like it's pretty common for positive reviews to be shorter. This could potentially drag down the overall average of positive reviews. However... there are more short positive reviews, but there are also more positive reviews in general, so maybe not.

38.7% of all negative reviews are 5 words or shorter, and 74.4% of all positive reviews are. That's quite a big chunk of reviews! 


### Length Stats

Word length histograms.... A LOT of reviews are all grouped together at the lower end of the spectrum

```{r}
#| label: pos-neg-revs
posrevs <- reviews_df |>
  filter(review_type=='POS')

negrevs <- reviews_df |>
  filter(review_type=='NEG')
```


```{r}
#| label: pos-neg-dist
pos <- ggplot(posrevs, aes(x=word_count)) + 
  geom_histogram(binwidth = 50)
pos

neg <- ggplot(negrevs, aes(x=word_count)) + 
  geom_histogram(binwidth = 50)
neg
```

Can I log transform this and then make box or violin plots? 



```{r}
#| label: WC-scatter
ggplot(reviews_df) +
  aes(x = review_type, y = word_count, color = review_type) +
  geom_jitter() +
  theme(legend.position = "none")
```

```{r}
#| label: WC-bplot
ggplot(reviews_df, aes(x=review_type, y=word_count)) + 
  geom_boxplot() 
  #geom_jitter(shape=16, position=position_jitter(0.2))
```

```{r}
#| label: WC-vplot
ggplot(reviews_df, aes(x=word_count, y=review_type)) + 
  geom_violin()
```



correlation between review length and review type?


## Word Types & Count

```{r}
#| label: GetTypes
remove_dupes <- function(x) {
  #split tokens to be iterated over
  words <- strsplit(x, " ")[[1]]
  #apply the unique() function per row
  unique_words <- unique(words)
  #re-collapse into rows
  paste(unique_words, collapse = " ")
}
```


```{r}
#| label: types-col
# get word types by removing duplicates from tokens rows
reviews_df$types <- sapply(reviews_df$tokens, remove_dupes)
```

```{r}
#| label: types-count
reviews_df <- reviews_df |>
  mutate(type_count = str_count(types, '\\,')+1)
```


## TTR 

TTR is used to measure the variety of language used in a text. It's not  exactly a measure of the complexity of a document, but can be considered with other factors as an indicator of a writer's language abilities. 

TTR is measured by dividing the total number of word types (unique words used) by total number of word tokens (all words used). A low score (closer to 0) indicates a highly repetitive document, and a high score (closer to 1) indicates a higher variety or words. A score of 1 would mean that no words were repeated in the document. 

TTR is very sensitive to document length. Too long, and documents taper off. There are only so many content words to be used in a document, eventually the highly repetitive function words will outnumber them. 

[source](https://medium.com/@rajeswaridepala/empirical-laws-ttr-cc9f826d304d)

```{r}
#| label: TTR
reviews_df <- reviews_df |>
  mutate(TTR = type_count/word_count)

reviews_df
```

### TTR Exploring 


## Emojis

A lot of work went into keeping those emojis, so I want to take a look at them! 

```{r}
#| label: emoji-str
emoji_pattern <- "[\\\U0001F600-\\\U0001F64F\\\U0001F300-\\\U0001F5FF\\\U0001F900-\\\U0001F9FF\\\U00002600-\\\U000027BF]"
```


```{r}
#| label: extract-emojis
emojis <- reviews_df |>
  filter(grepl(emoji_pattern, types, perl=TRUE)) |>
  mutate(types = str_remove_all(types, "\\w|,|'|‘|’
                                 %|\\.|’|\\\036|－|❝|❞
                                 |‑"))

emojis
```

Only 1,622 of our reviews contain some kind of emoji usage. Not as many as I expected!


```{r}
#| label: emojis-only
emoji_df <- emojis |>
  mutate(types = sapply(emojis$types, remove_dupes)) |>
  count(types, sort=TRUE)

emoji_df
```


```{r}
#| label: plot-emoji
#| eval: FALSE
max_count <- emoji_df |>
  pull(n) |>
  max()


hchart(
  emoji_df, 
  "bar",
  hcaes(types, n), 
  name = "Count",
  # dataLabels = list(enabled = TRUE, style = list(fontWeight = "normal"))
  ) |>
  hc_xAxis(
    min = 0,
    max = 30,
    scrollbar = list(enabled = TRUE)
    ) |>
  hc_yAxis(
    max = max_count,
    title = list(
      text = "Count",
      align = "high"
      )
    ) |>
  hc_tooltip(
    headerFormat = "{point.key}",
    pointFormat = " {point.y}"
  ) |>
  hc_size(height = 700)
```


Cool [source](https://jkunst.com/blog/posts/2021-01-10-using-emoji-with-highcharterhighcharts/) for the emoji finding & charting code.



## Tf-idf

Calculating Tf-idf for our game reviews (excluding emoji usage for this one).

```{r}
#| label: rm-stopwords
data(stop_words)

tokens_df <- reviews_df |>
  unnest_tokens(word, review) |>
  anti_join(stop_words)

tokens_df <- tokens_df |>
  count(word, sort = TRUE) 

tokens_df
```


By tokenizing and removing stop words, which in this case also removed all emojis and special characters, we went from around 11 million tokens (from [data cleanup tokenizing](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/1-data-cleanup.md#tokenizing)) down to 3.6 million (from calculating sum(n) on tokens_df).

The most common word token among these game reviews.... is game! 

Looking through the first few pages of tokens, I wonder if we can pick out some key words of game elements reviewers find important enough to comment on specifically. I see story, gameplay, characters, hours (game length, probably), worth (game price, I bet), puzzles, pretty, money, music, graphics... all in the top 50 words. 

What about top words per review type? 

```{r}
#| label: pos-topwords
pos_toks <- posrevs |>
  unnest_tokens(word, review) |>
  anti_join(stop_words) |>
  count(word, sort = TRUE)
pos_toks
```

```{r}
#| label: neg-topwords
neg_toks <- negrevs |>
  unnest_tokens(word, review) |>
  anti_join(stop_words) |>
  count(word, sort = TRUE)
neg_toks
```

How can I visually compare these two sets of data? 

### TF

calculate the frequency for each word for the works of Jane Austen, the Brontë sisters, and H.G. Wells by binding the data frames together


```{r}
#| label: rev-freq
frequency <- tokens_df |>
  mutate(proportion = n / sum(n)) |>
  select(-n)

frequency
```

This means that 64% of all the words in game reviews is the word game???


### Plotting

```{r}
#| label: pos-neg-freq
freq <- bind_rows(mutate(pos_toks, review_type = "Positive"),
                       mutate(neg_toks, review_type = "Negative")) |> 
  mutate(word = str_extract(word, "[a-z']+")) |>
  count(review_type, word) |>
  group_by(review_type) |>
  mutate(proportion = n / sum(n)) |> 
  select(-n) |> 
  pivot_wider(names_from = review_type, values_from = proportion) |>
  pivot_longer(Positive:Negative,
               names_to = "review_type", values_to = "proportion")

freq
```


### In progress: similar tokens pos & neg

```{r}
#| label: similar-toks
#| eval: false
ggplot(freq, aes(x = proportion, y = review_type, 
                      color = abs(proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~review_type, ncol = 2) +
  theme(legend.position="none") 
```

+
  labs(y = "Jane Austen", x = NULL)


Words closest to the line have similar frequencies in both samples

Also:
not all the words are found in all three sets of texts and there are fewer data points in the panel for Austen and H.G. Wells.

quantify how similar and different these sets of word frequencies are using a correlation test

### In progress: correlation review type

```{r}
#| label: austen-corr
#| eval: false
cor.test(data = freq[freq$review_type == "Positive",],
         ~ proportion + Negative)


#cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         #~ proportion + `Jane Austen`)
```


### tf-idf notes from class

how to quantify what a document is about
 
tf - term frequency - how frequently a word shows up
idf - inverse document frequency- how many documents a word appears in (no of docs / no of dccs containing term)

tf-idf: frequency of a term adjusted for how rarely it is used

- intended to measure how important a word is to a document in a collection (or corpus) of documents

### term freq

```{r}
#| label: revtoks-totals
review_words <- reviews_df |>
  unnest_tokens(word, review) |>
  count(review_type, word, sort = TRUE)

total_words <- reviews_df |> 
  group_by(review_type) |> 
  summarize(total = sum(word_count))

review_words <- left_join(review_words, total_words)

review_words
```

Calculating the total word counts per word and by review type and comparing to the total word counts by review type. With these, we can get moving on tf-idf.


```{r}
#| label: revtoks-freq
ggplot(review_words, aes(n/total, fill = review_type)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~review_type, ncol = 2, scales = "free_y")
```

### bind_tf_idf() function

find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents

- which words are the words that define the text?

- what words are common (but not too common)?

bind_tf_idf()

takes a tidy text dataset as input with one row per token (term), per document

  - one column contains the terms/tokens (word)
  - one column contains the documents (review type)
  - last necessary column contains the counts, how many times each document contains each term (n)
  
```{r}
#| label: review-tfidf
review_tf_idf <- review_words |>
  bind_tf_idf(word, review_type, n)

review_tf_idf
```

Some notes to pay attention to:

idf and thus tf-idf are zero for these extremely common words
  - words that appear in all six of austen's novels
  - this decreases the weight for these extremely common words
  
idf will be a higher number for words that occur in fewer of the documents in the collection

To take a look at high idk terms:

```{r}
#| label: aust-high-tfidf
#| eval: false
book_tf_idf %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

proper nouns! names. Makes sense, generally a character only appears in one book and would be ver informative of that book. 

```{r}
#| label: aust-propnouns
#| eval: false
book_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

We can conclude: jane austen uses a lot of the same language between her books, and the thing that really distinguishes them is the characters in them and locations.



