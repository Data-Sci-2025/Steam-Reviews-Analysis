---
title: "2-data-analysis"
format: gfm
knitr:
  opts_chunk:
    fig.path: "../images/"


---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(tidytext)
library(ggplot2)
library(tidyr)
library(scales)
library(forcats)
```

**Note**: Please run [0-data-exploration](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/0-data-exploration.qmd) and [1-data-cleanup](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/1-data-cleanup.qmd) first to create the version of the .csv file needed to start here.


## A note about the data

There are two main aspects of my data being analyzed in this file. 

1. review type

Reviews in Steam can either be positive or negative (indicated by a thumbs up or thumbs down). I will often refer to review type in relation to other variables. There are always only two categories this could be. 

2. game rank

Game ranks in Steam are a bit more complicated. There are nine total ranks a game could fall into, Overwhelmingly Positive, Very Positive, Positive, Mostly Positive, Mixed, and the mirrored version of the first four for Negative games as well. 

There isn't an official source of the exact breakdown of how these ranks are calculated, but people have worked out that it comes down to both the ratio of positive and negative review types as well as total number of reviews. 

For example, a game with 3,000 99% positive reviews will still be ranked lower than a game with 5,000 95% positive reviews. Or a game can be 100% positive, but if it has fewer than 50 reviews, it will be stuck in the "Positive" ranking. As far as I've understood, these calculations are the same for negatively reviewed games. It appears that regardless of number of reviews, if the ratio of positive/negative reviews is in the 40%-60% range, it will be considered "Mixed".

I will, at times during my analysis, lump these game rankings together into a simplified three tiers; positive, mixed, and negative. Just know, that in these moments, the positive and negative options are referring to four smaller categories grouped together for the sake of ease. 



```{r}
#| label: readingin
# reviews 
full_df <- read_csv("../private/reviews_analyze.csv", show_col_types = FALSE)

# game titles, ranks, etc, we'll need it later
games <- read_csv("../notes_and_info/0-gameinfo.csv", show_col_types = FALSE)
```


## Downsampling Data

In the data as I gathered it, positively reviewed games were quite over represented in the data. Because many of the positively reviewed games in the data are widely beloved games, they naturally have only gathered more and more reviews with time, with a majority of those being positive reviews. 

The number of reviews (total and in English) at [this table](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/notes_and_info/0-gameinfo.csv) were added manually by me using the total listing on the Steam App. Those numbers are accurate as of the date I downloaded my reviews (shown in my [projnotes.md](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/notes_and_info/projnotes.md)). However the script I used to download the reviews would time out after a certain point, and not every single review was downloaded. 

The original numbers are shown here.


```{r}
#| label: full-revs-total
full_df |>
  group_by(steam_id) |>
  summarise(total = n()) |>
  arrange(desc(total))

full_df |>
  group_by(review_type) |>
  summarise(total = n())
```


My theory here just comes down to popularity. As a game releases and players try it out and review it, it begins to migrate through the ranks of either positive or negative. As new players find out about the game, if it shows already that it's being negatively reviewed, why would they spend money to try it out themselves? I wouldn't! On the other hand, if players see that a game is getting positive reviews, they're more likely to try it and, in turn, also positively review it.

The negatively reviewed games were very likely largely forgotten to time once they started the descent into the negative review rankings. After an initial flood of bad reviews, feedback fell off. 

So, there it is. Positively ranked games have overall more reviews than negatively ranked games, and there are more positive reviews than negative ones in my data as a result. 


Because of this, we decided the best fit would be a stratified downsampling of the total positive reviews. Positive reviews to be randomly, but proportionally, cut down in total to be made more equal to negatively reviewed games and as a result, more appropriately comparable. 


```{r}
#| label: revs-split
posreviews <-
  full_df |>
  filter(review_type=="POS")

negreviews<-
  full_df |>
  filter(review_type=="NEG")
```



```{r}
#| label: downsampling
# seed so randomly selected downsampled reviews will be the same
set.seed(1234)

# reviews to be analyzed, sampled proportionally by difference in neg & pos reviews per game
reviews_df <- posreviews |>
  slice_sample(prop=nrow(negreviews)/nrow(posreviews), by=steam_id)

reviews_df <- bind_rows(reviews_df, negreviews)

# a new df, much more manageable in size
reviews_df
```


```{r}
#| label: merged-games

#combine the reviews df and the game info df for all info needed
merged_games <- reviews_df |>
  left_join(games) 
```



## Getting a Look at the Data

The data we'll be looking at is game reviews for 45 games spread out over 15 years from 2008 to 2024 (no games from 2021 are in the data). To get a feel for what we're looking at, let's take a look.

```{r}
#| label: game-rank-yr
simple_games <- games |>
  #simplify the 9 game ranks into three by combining all positives and negatives
  mutate(rank = str_replace_all(rank, "\\w+ (\\w)", "\\1")) |>
  mutate(year_range = cut(year, 
          breaks=c(0,2011,2014,2017,2020,2024),
          labels=c('2008-2011','2012-2014','2015-2017','2018-2020','2022-2024')))

ggplot(simple_games, aes(x = year_range, fill=rank)) +
      geom_bar(position="dodge") +
      labs(title = "Number of Games per Rank by Year",
           x = NULL,
           y = "Count") +
      geom_text(stat = "count", aes(label = after_stat(count)), size = 3.5, vjust = 2, hjust = 0.5, position = position_dodge(width=.9)) +
      theme_minimal()
```

As mentioned above, the positive and negative "ranks" here are 4 ranks each combined. Are games getting better over time? This sample of 45 games seems to suggest it! That's not something I want to conclude from such a small sample size of all Steam games, but it's an interesting look here. 


```{r}
#| label: merged2
# merging full size reviews_df to games info
merged_games2 <- full_df |>
  left_join(games) 
```


```{r}
#| label: revs-yr
games_year <- merged_games |>
  mutate(year_range = cut(year, 
            breaks=c(0,2011,2014,2017,2020,2024),
            labels=c('2008-2011','2012-2014','2015-2017','2018-2020','2022-2024')))

ggplot(games_year, aes(x = year_range, fill=review_type)) +
      geom_bar(position="dodge") +
      labs(title = "Number of Reviews by Year (adjusted)",
           x = "Years",
           y = "Count") +
      theme_minimal()

games_year2 <- merged_games2 |>
  mutate(year_range = cut(year, 
            breaks=c(0,2011,2014,2017,2020,2024),
            labels=c('2008-2011','2012-2014','2015-2017','2018-2020','2022-2024')))

ggplot(games_year2, aes(x = year_range, fill=review_type)) +
      geom_bar(position="dodge") +
      labs(title = "Number of Reviews by Year (original)",
           x = "Years",
           y = "Count") +
      theme_minimal()
```

Again, these year divisions contain 8 games, 9 games, 5 games, 11 games, and 12 games each. The number and ratio of negative and positive reviews for games across 15 years seems to align with the chart above. In this data, at least, games got more positive with time!



### Reviews per game
 
```{r}
#| label: rev2
revcount_df <- merged_games2 |>
  group_by(game) |>
  summarise(total = n()) |>
  arrange(desc(total))
revcount_df
```

```{r}
#| label: split-data
cut1 <- revcount_df |>
  slice(1:10) |>
  #reorder by total so that the plot legend is in the right order
  mutate(game = fct_reorder(game, total, .desc = TRUE))

cut2 <- revcount_df |>
  slice(11:20) |>
  mutate(game = fct_reorder(game, total, .desc = TRUE))

cut3 <- revcount_df |>
  slice(21:29) |>
  mutate(game = fct_reorder(game, total, .desc = TRUE))

cut4 <- revcount_df |>
  slice(30:36)|>
  mutate(game = fct_reorder(game, total, .desc = TRUE))

cut5 <- revcount_df |>
  slice(37:45) |>
  mutate(game = fct_reorder(game, total, .desc = TRUE))
```


```{r}
#| label: revcount-bars
ggplot(cut1, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")

ggplot(cut2, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")

ggplot(cut3, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")

ggplot(cut4, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")

ggplot(cut5, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")
```

Take care to note the y-axis and how it changes between plots. The games with the fewest reviews are significantly less than the ones with the most. This is because of how the "Positive" and "Negative" categories are calculated, the cutoff point at those ranks is 50 reviews. Any more than that and the game will migrate to a different rank. 

Also please note that the numbers indicated here are not reflective of the actual number of reviews as shown on Steam (documented [here](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/notes_and_info/0-gameinfo.csv)), and only reflect the total number of reviews in *this* data set. The script used to download would time out on some of the more reviewed games. 

Finally, I've mentioned before that it is the most positively reviewed games that had the most total reviews total, and that is *mostly* true. Resident Evil 6 is quite high in the total numbers of reviews per game, but is still in the Mixed reviews ranking, lots of opinionated people who can't come to a consensus! Otherwise the top 10 in number of reviews are high in the Steam ranks. The least reviewed games are, of course, those in the plain Positive and Negative ranks, thanks to the limit in those categories being up to 50 reviews. 


Just to get a full scope of the scale (and why I split the data up like I did above) here are all the totals in comparison.


```{r}
#| label: revs-by-game
ggplot(revcount_df, aes(x = reorder(game, -total), y=total)) +
  geom_bar(stat='identity') +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y="Total", title="Total Reviews per Game")
```

### Positive and Negative

How many reviews of each type are there?

```{r}
#| label: plot-reviews
ggplot(full_df, aes(x=review_type, fill=review_type )) + 
  geom_bar( width = 0.5) +
  scale_fill_brewer(palette = "Set2") +
  geom_text(stat = "count", aes(label = after_stat(count)), size = 3.5, vjust = 3, hjust = 0.5, position = "stack") +
  theme(legend.position="right") +
  labs(x="Review Type", y="Count", title="Total Reviews per Type (original)")
```

```{r}
#| label: plot-reviews-adj
ggplot(reviews_df, aes(x=review_type, fill=review_type )) + 
  geom_bar( width = 0.5) +
  scale_fill_brewer(palette = "Set2") +
  geom_text(stat = "count", aes(label = after_stat(count)), size = 3.5, vjust = 3, hjust = 0.5, position = "stack") +
  theme(legend.position="right") +
  labs(x="Review Type", y="Count", title="Total Reviews per Type (adjusted)")
```


Let's look at the actual count of reviews per game ranking, simplified down to only positive, negative, and mixed for ease.


```{r}
#| label: rank-rev-count
merged_games |>
  group_by(rank) |>
  #merging all varieties of pos/neg together by removing the first word per rank
  #very positive and overwhelmingly positive both just become positive etc
  mutate(rank = str_replace_all(rank, "\\w+ (\\w)", "\\1")) |>
  summarise(count = n())

```

Even after adjusting, Positive ranked games are more reviewed than Negative ones! Some of those games had a 99% ratio of positive reviews, so even after being proportionally downsampled, there are a lot left!



## Word Count

Word count per review for the downsampled data:

```{r}
#| label: word-count
reviews_df <- reviews_df |>
  mutate(word_count = str_count(tokens, '\\,')+1)

reviews_df
```


Doing the same on the full data set so we can look at the true average length of *all* reviews

```{r}
full_df <- full_df |>
  mutate(word_count = str_count(tokens, '\\,')+1)
```



### Review length

With word counts per review added, let's take a look!

```{r}
#| label: WC-stats
summary(reviews_df$word_count)
```

```{r}
#| label: WC-full
summary(full_df$word_count)
```

So the average review (for the full data) is around 51/52 words long, and the median 17 words. 
Adjusted, that shifts to an average of almost 62 words, and a median of 22. 

Let's look a little deeper into the slight shift. 


```{r}
#| label: short-long-counts
full_df |>
  filter(word_count==1) |>
  group_by(review_type) |>
  summarise(total=n())

full_df |>
  filter(word_count>1700) |>
  group_by(review_type) |>
  summarise(total=n())
```


A huge amount of positive reviews (8%!) are only one word long. For negative reviews that total is 3%. That would certainly explain the dip in average length when looking at the full data set! 

Let's take a look at those short and long reviews.


```{r}
#| label: WC-low-high
full_df |>
  filter(word_count==1)

full_df |>
  filter(word_count>1700)
```

There's quite a few one-word reviews! Some of them are pretty reasonable... "Amazing", "Spooky", "Boring", "Unplayable". Short and sweet, gets the point across well enough. Others are a little less obvious. I saw a number of keyboard smash reviews, strings of numbers, I saw one that was just a rabbit emoji. The last one might be a reference I just don't understand.

Looking next at these longest reviews, I was really surprised! A huge majority of these are exactly the same phrase repeated over and over, and those repeated reviews are all for the same game as well. I have to do some investigation... 

```{r}
#| label: the-end-is
reviews_df |>
  filter(review_id==198792433) |>
  select(review)
```


Checking my game info [here](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/notes_and_info/0-gameinfo.csv), I was able to confirm that these repeating reviews are all from "The Stanley Parable: Ultra Deluxe". 

Looking further, I found some more information. ["The end is never"](https://thestanleyparable.fandom.com/wiki/The_End_Is_Never...) is a tagline for the game itself, and appears within the game multiple times as a reference to the inescapable time loop the game's protagonist is stuck in. The phrase also apparently appears on the game's loading screens in a constant loop. 


According to [this](https://rebeccagraf-63084.medium.com/how-long-should-book-reviews-be-250187486779) Medium article, somewhere in the 400 word range seems to be considered the "ideal" length for a book review. 100-400 words for maybe a small, personal review, or 400-600 for a professional one. The expectations for video game reviews are not so different.

[This](https://meliorgames.com/best-practices/the-ultimate-guide-to-writing-a-game-review/) guide to writing game reviews covers a lot of factors surrounding the game, deadlines, and expectations. It looks like official game review publishing sites (like IGN, Polygon, etc.) have an ideal length of ~1,000 words for a review. It could just come down to price - buying a video game costs more than buying a book (generally) and players will want to know ahead of time what they're committing to. The guide also acknowledges that user reviews on sites like Steam, like what are being analyzed here, are likely to be shorter. 



### Average Review Length

Now to what we came here to look into. Are positive reviews typically longer, or negative reviews? 

**Note:** These are all calculated on the *full size* data set.


```{r}
#| label: plot-avg-len
ggplot(full_df, aes(x=review_type, y = word_count)) + 
  stat_summary(fun = mean, geom = "bar", fill = "skyblue") +
  geom_text(aes(label = after_stat(sprintf("%.2f", y))), stat = "summary", fun = "mean", vjust = 3, hjust=0.5) +
  labs(y="Word Count", x="Review Type", title="Average Review Length")
  theme(legend.position="right")
```

Now that's really interesting! Despite all of the longest reviews we looked at above being positive, negative reviews are still quite a bit longer on average. People must have a lot more to say when they dislike a game than when they like one!

I wonder if there's a trend in shorter reviews that might be swinging this average one way or the other. 

```{r}
#| label: rev-type-count
full_df |>
  group_by(review_type) |>
  filter(word_count<6) |>
  summarise(word_count = sum(word_count))
```

Looks like it's pretty common for positive reviews to be shorter. This could potentially drag down the overall average of positive reviews. 

38.7% of all negative reviews are 5 words or shorter, and 74.4% of all positive reviews are. That's quite a big chunk of reviews! Definitely significant enough to make the average length of positive reviews quite short. 


Rather than look at the average review length of all 45 games, let's look at maybe the longest 5 and shortest 5, just to see. 

```{r}
#| label: long-short-cuts
# recreate merged games2 so it includes word count
# reminder that merged games 2 is with the full data set
merged_games2 <- full_df |>
  left_join(games)

LS_games <- merged_games2 |>
  group_by(game) |>
  summarise(avg = mean(word_count)) |>
  arrange(desc(avg))

#pull the game steam rankings per title
gamerank <- merged_games2 |> dplyr::select(game,rank)

LS_games <- left_join(LS_games, gamerank)

# longest reviews 5 games
LScut1 <- LS_games |>
  distinct(game, .keep_all = TRUE) |>
  slice(1:5) |>
  #reorder by total so that the plot legend is in the right order
  mutate(game = fct_reorder(game, avg, .desc = TRUE))

# shortest reviews 5 games
LScut2 <- LS_games |>
  distinct(game, .keep_all = TRUE) |>
  slice(41:45) |>
  mutate(game = fct_reorder(game, avg, .desc = TRUE))
```


```{r}
#| label: long-short-bars
ggplot(LScut1, aes(x = reorder(game, -avg), y=avg, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  geom_text(aes(label=rank), vjust=2, hjust=1, angle = 70) +
  labs(x="Game", y = "Avg. Review Length", title="Longest Reviews")

ggplot(LScut2, aes(x = reorder(game, -avg), y=avg, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  geom_text(aes(label=rank), vjust=2, hjust=1, angle = 70) +
  labs(x="Game", y = "Avg. Review Length", title="Shortest Reviews")

```


It looks like maybe the "people have more to say about games they don't like" theory might be onto something. None of the longest review length averages touch the four positive game ranks. Most of them are Mixed! While the mixed ranking is calculated by the ratio of positive to negative reviews per game, it's not a stretch to imagine the people reviewing it may also feel mixed and have more to say in response. 

Almost all of the games with the shortest reviews are in the positive rankings! 



### Length Stats

Review length histograms.... A LOT of reviews are all grouped together at the lower end of the spectrum as we just saw. In the case of positive reviews, almost 3/4 of all reviews are 5 words or shorter! 

To make the histogram easier to review, a log transformation has been used.


```{r}
#| label: pos-neg-revs
posrevs <- full_df |>
  filter(review_type=='POS')

negrevs <- full_df |>
  filter(review_type=='NEG')
```


```{r}
#| label: pos-neg-dist
pos <- ggplot(posrevs, aes(x=word_count)) + 
  geom_histogram(binwidth = 50) +
  scale_y_log10() +
  labs(title="Positive Review Length", x="Review Length", y="Count")
pos

neg <- ggplot(negrevs, aes(x=word_count)) + 
  geom_histogram(binwidth = 50) +
  scale_y_log10() +
  labs(title="Negative Review Length", x="Review Length", y="Count")
neg
```



```{r}
#| label: WC-vplot
ggplot(full_df, aes(x=review_type, y=word_count, fill=review_type)) + 
  geom_violin() +
  scale_y_log10() +
  labs(x="Review Type", y="Word Count", title="Word Count by Review Type") 
```

An interesting visual! The lower length reviews appear in *very* similar proportions in both categories, but more extreme in the positive reviews. The higher average review length for negative reviews is visible at the widest point. 


## Word Types & Count

```{r}
#| label: GetTypes
remove_dupes <- function(x) {
  #split tokens to be iterated over
  words <- strsplit(x, " ")[[1]]
  #apply the unique() function per row
  unique_words <- unique(words)
  #re-collapse into rows
  paste(unique_words, collapse = " ")
}
```


```{r}
#| label: types-col
# get word types by removing duplicates from tokens rows
full_df$types <- sapply(full_df$tokens, remove_dupes)
```

```{r}
#| label: types-count
full_df <- full_df |>
  mutate(type_count = str_count(types, '\\,')+1)
```


## TTR 

TTR is used to measure the variety of language used in a text. It's not exactly a measure of the complexity of a document, but can be considered with other factors as an indicator of a writer's language abilities. 

TTR is measured by dividing the total number of word types (unique words used) by total number of word tokens (all words used). A low score (closer to 0) indicates a highly repetitive document, and a high score (closer to 1) indicates a higher variety or words. A score of 1 would mean that no words were repeated in the document. 

TTR is very sensitive to document length. Too long, and documents taper off. There are only so many content words to be used in a document, eventually the highly repetitive function words will catch up or potentially outnumber them. 

[source](https://medium.com/@rajeswaridepala/empirical-laws-ttr-cc9f826d304d)

```{r}
#| label: TTR
full_df <- full_df |>
  mutate(TTR = type_count/word_count)

full_df
```



### TTR Exploring 

I've covered how TTR is sensitive to long documents, but it's also the case that it can be sensitive to short documents as well. A 5 word review is unlikely to have many repeated words and will have a very high TTR score, indicating a high rate of language diversity. 

Let's see if we can pinpoint where TTR falls off in this data


```{r}
#| label: TTR-line
ggplot(full_df, aes(x=word_count, y=TTR)) +
  geom_line() +
  geom_smooth() +
  lims(x = c(1, 1800))
```

This plot looks crazy, but it still does give some idea of where the trend begins to plateau, a good sign of where TTR starts to be affected by the length of the document. At the far end, of course, are those 2000 word reviews that just say "the end is never" over and over again. 

Let's look at reviews in the 200-600 word range to give TTR a good look. I selected this length based on the line plot above aiming for a point where the trend wasn't *so* steep nor fully out to a plateau. 

```{r}
#| label: midlen-revs
midlength <- full_df |>
  filter(word_count>200, word_count<600)

midlength
```


```{r}
summary(midlength$TTR)
```

```{r}
midpos <- midlength |>
  filter(review_type=='POS')
summary(midpos$TTR)

midneg <- midlength |>
  filter(review_type=='NEG')
summary(midneg$TTR)
```
```{r}
nrow(midpos)
nrow(midneg)
```

First, there are positive reviews in this midlength selection of reviews. Makes sense considering we know there are so many more positive reviews in the full dataset, which this is made from. As a note, when run on the downsampled dataset, there were much more negative reviews in this midlength selection. I put this up to the fact that we know negative reviews are longer on average.

Let's select a random set of 2,953 of midlength positive reviews and compare a bit more equally.

```{r}
set.seed(60)
midpos <- midpos |>
  slice_sample(n = 2953)

summary(midpos$word_count)
summary(midneg$word_count)
```

Just to take a look at the total words being compared here, things look just about as expected! Both review sets run the full span of the length selection. Even here, positive reviews have a shorter average length.


```{r}
summary(midpos$TTR)
summary(midneg$TTR)
```

The average TTR score for reviews in the 200-600 word range is not too significantly different for positive vs negative reviews even after slicing into equal sample sizes. Positive reviews are just a *bit* lower in average in TTR than negative reviews, showing a little bit more repetitiveness. 

What is interesting, though, is the difference in range of TTR per review type. Positive reviews have a difference of 0.744454 and negative reviews 0.807027. Negative reviews are more extreme on both the low and high scoring ends of TTR, indicating more variety in how repetitive (or not) negative reviews are. 


## Tf-idf

Calculating Tf-idf for our game reviews. Because they were creating a lot of issues with tokenizing and odd characters, I've excluded emojis and gone simply for text reviews. It's a bummer to lose out on such strong reviews as "one single poop emoji", but it needs to be done for ease of processing. For now, at least.

```{r}
#| label: rm-stopwords
data(stop_words)

tokens_df <- reviews_df |>
  unnest_tokens(word, review) |>
  anti_join(stop_words)

tokens_df <- tokens_df |>
  count(word, sort = TRUE) 

tokens_df
```


By tokenizing and removing stop words, which in this case also removed all emojis and special characters, we went from around 11 million tokens (from [data cleanup tokenizing](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/1-data-cleanup.md#tokenizing)) down to 3.6 million before downsampling, and 1.7 million after (from running sum(n) on tokens_df).


The most common word token among these game reviews.... is game! 

Looking through the first few pages of tokens, I wonder if we can pick out some key words of game elements reviewers find important enough to comment on specifically. I see story, gameplay, characters, hours (game length, probably), worth (game price, I bet), puzzles, pretty, money, music, graphics... all in the top 50 words. 


What about top words per review type? 


```{r}
#| label: pos-topwords
pos_toks <- posrevs |>
  unnest_tokens(word, review) |>
  anti_join(stop_words) |>
  count(word, sort = TRUE)
pos_toks
```

Taking a look at the most frequent words in positive reviews can give us a look into what people might write about enjoying specifically! Story, experience, characters, gameplay, puzzle, horror (?!), world, art.... And what they're considering in their reviews! I see worth in there pretty high up, hours
, unique.



```{r}
#| label: neg-topwords
neg_toks <- negrevs |>
  unnest_tokens(word, review) |>
  anti_join(stop_words) |>
  count(word, sort = TRUE)
neg_toks
```

What are people writing about in negative reviews? I can pick out at a glance time, bad, money, worst, terrible.

But there are a lot of overlapping words between the two, pretty high up in frequency for both. I came into this project with the suspicion that a classifier would have some more trouble than maybe expected, but this is another nudge in that direction for me. 

Game, play, story, games, fun, and time are in top ten tokens for both positive and negative reviews. That's more than half of those top tokens in common.



### Term Frequencies


```{r}
#| label: rev-freq
frequency <- tokens_df |>
  mutate(proportion = n / sum(n)) |>
  select(-n)

frequency
```

This means that 6.3% of all the words in game reviews is the word game. 

```{r}
#| label: rev-types-prop
posfreq <- pos_toks |>
  mutate(proportion = n / sum(n)) |>
  select(-n)
posfreq

negfreq <- neg_toks |>
  mutate(proportion = n / sum(n)) |>
  select(-n)
negfreq
```


```{r}
#| label: pos-neg-freq
freq <- bind_rows(mutate(pos_toks, review_type = "Positive"),
                       mutate(neg_toks, review_type = "Negative")) |> 
  mutate(word = str_extract(word, "[a-z']+")) |>
  count(review_type, word) |>
  group_by(review_type) |>
  mutate(proportion = n / sum(n)) |> 
  select(-n) |> 
  pivot_wider(names_from = review_type, values_from = proportion) |>
  pivot_longer(Positive:Negative,
               names_to = "review_type", values_to = "proportion")

freq
```



### Total Words & Freq


```{r}
#| label: revtoks-totals
review_words <- reviews_df |>
  unnest_tokens(word, review) |>
  anti_join(stop_words) |>
  count(review_id, word, sort = TRUE)

total_words <- review_words |> 
  group_by(review_id) |> 
  summarize(total = sum(n))

review_words <- left_join(review_words, total_words)

review_words <- left_join(reviews_df, review_words)

review_words <- review_words |>
  select(review_id, review_type, word, n, total)

review_words
```

Calculating the total word counts per word and per review. With these, we can get moving on tf-idf.



### Bind_tf_idf()

To re-define tf-idf from our class activity: find the important words for the content of each document (game review for us) by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents

- which words are the words that define the text?

- what words are common (but not too common)?

  
```{r}
#| label: review-tfidf
review_tf_idf <- review_words |>
  bind_tf_idf(word, review_id, n)

review_tf_idf
```

with stop words included there are 6,412,302 total rows
with stop words excluded it went down to 3,045,610!
After downsampling (and stop words removed), it's down to 1,430,815!

```{r}
#| label: high-count-words
review_tf_idf |>
  arrange(desc(n))
```


```{r}
reviews_df |>
  filter(review_id==76036759)
```




To prep this dataframe for the classifier, let's narrow down to only the columns we know we want.

```{r}
#| label: tf-df-final
review_tf_idf <- review_tf_idf |>
  select(review_id, review_type, word, total, tf_idf)

review_tf_idf
```



```{r}
#| label: write_tf-idf-csv
write_csv(review_tf_idf, file="../private/reviews_tfidf.csv")
```

