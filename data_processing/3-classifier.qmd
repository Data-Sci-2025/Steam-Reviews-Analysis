---
title: "3-classifier"
format: gfm
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(caret)
library(mlbench)
library(klaR)
library(naivebayes)
```

**Note**: Please run [0-data-exploration](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/0-data-exploration.qmd), [1-data-cleanup](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/1-data-cleanup.qmd), and [2-data-analysis](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/2-data-analysis.qmd) first to create the version of the .csv file needed to start here.

## Classifier Data


To get another look at the df to be used to train the classifier later. This is the tf-idf information for the downsampled version of the game reviews data, stop words removed. 


```{r}
review_tf_idf <- read_csv("../private/reviews_tfidf.csv", show_col_types = FALSE)
```

```{r}
review_tf_idf
```


Now if I'm going to try to predict 





### Remove NA 

Just to double check and clean out any troublesome NA values:


```{r}
colSums(is.na(review_tf_idf))
```

```{r}
review_tf_idf <- na.omit(review_tf_idf)
```

```{r}
colSums(is.na(review_tf_idf))
```


```{r}
review_tf_idf |>
  group_by(review_type) |>
  summarise(count=n())
```





## Data Setup

Even after downsampling and removing stop words, I ran into issues trying to pivot my very large dataframe wider. 

Initially I sampled lines 1:400,000 of my data which worked well enough! But when I started to train the classifier on it, ran into an issue where even in all those rows, only POS marked reviews were appearing. 

Instead, I did a random sampling method of 300,000 rows of data and confirmed afterward that it contained both NEG and POS marked reviews. 


```{r}
set.seed(99)
setup_sample <- sample_n(review_tf_idf, 300000)

setup_sample <-setup_sample |>
  rename(WC=total)
  
setup_sample
```

```{r}
setup_sample |>
  group_by(review_type) |>
  summarise(count=n())
```

Pivot wider so that all the individual words appear as column names, populated below by tf-idf values per review. 

```{r}
setup_sample <- setup_sample |>
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0)

ncol(setup_sample)
```

Because of how the classifier works, any word that only appears once in any of the reviews (has only one tf-idf value over 0) would be discarded. If we train a classifier on the word "refund" and that word only appears once in this data subset, it would never encounter the word "refund" in the testing data to apply that classification weight to. It's wasted computing space and power on a word that it will never see again. 

To get rid of them in advance and ease the classification training process, this chunk will discard any column (word) that only has one value greater than 0 - any word that only appears a single time.


```{r}
setup_sample <- setup_sample |>
  purrr::discard(function(x) sum(x != 0) == 1)

ncol(setup_sample)
```



## Building a Classifier

Split the data by review type into training and testing data, with 75% of the data going to training.


```{r}
# split into training and testing sets

inTrain <- createDataPartition(
  y = setup_sample$review_type,
  ## the outcome data are needed
  p = .75,
  ## The percentage of data in the
  ## training set
  list = FALSE
)


```


```{r}
training <- setup_sample[ inTrain,]
testing  <- setup_sample[-inTrain,]

nrow(training)
ncol(training)

nrow(testing)
ncol(testing)
```

The classifier will be trained on 41,000 reviews and tested on close to 14,000 to see how well it can, by words used in a review, predict if a review itself is a positive or negative one.


## Training

I initially attempted to train on a larger subset of 400,000 samples, but the training ran for 6+ hours without end and I decided to downsize to 300,000. This training I ran overnight, so I don't know how long it actually took.


```{r}
ctrl <- trainControl(method = "repeatedcv", repeats = 3)

nb_model = train(
  review_type ~ .- review_id - WC, # Specifying the response variable and the feature variables
  method = "naive_bayes", # Specifying the model to use
  data = training, 
  trControl = ctrl
)
```



## Testing

```{r}
nb_test <- predict(nb_model, newdata = testing)
str(nb_test)
```

```{r}
nb_probs <- predict(nb_model, newdata = testing, type = "prob")
head(nb_probs)
```


```{r}
confusionMatrix(nb_test, as.factor(testing$review_type))
```













