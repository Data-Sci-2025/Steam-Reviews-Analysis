---
title: "2-data-analysis"
format: gfm
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(tidytext)
library(ggplot2)
library(tidyr)
library(scales)
library(ggplot2)
library(forcats)
```

**Note**: Please run [0-data-exploration](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/0-data-exploration.qmd) and [1-data-cleanup](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/1-data-cleanup.qmd) first to create the version of the .csv file needed to start here.


## A note about the data

There are two main aspects of my data being analyzed in this file. 

1. review type

Reviews in Steam can either be positive or negative (indicated by a thumbs up or thumbs down). I will often refer to review type in relation to other variables. There are always only two categories this could be. 

2. game rank

Game ranks in Steam are a bit more complicated. There are nine total ranks a game could fall into, Overwhelmingly Positive, Very Positive, Positive, Mostly Positive, Mixed, and the mirrored version of the first four for Negative games as well. 

There isn't an official source of the exact breakdown of how these ranks are calculated, but people have worked out that it comes down to both the ratio of positive and negative review types as well as total number of reviews. 

For example, a game with 3,000 99% positive reviews will still be ranked lower than a game with 5,000 95% positive reviews. Or a game can be 100% positive, but if it only has up to 50 reviews, it will be stuck in the "Positive" ranking. As far as I've understoon, these calculations are the same for negatively reviewed games. It appears that regardless of number of reviews, if the ratio of positive/negative reviews is in the 40%-60% range, it will be considered "Mixed".

I will, at times during my analysis, lump these game rankings together into a simplified three tiers. Positive, mixed, and negative. Just know, that in these moments, the positive and negative options are referring to four smaller categories grouped together for the sake of ease. 



```{r}
#| label: readingin
# reviews 
full_df <- read_csv("../private/reviews_analyze.csv", show_col_types = FALSE)

# game titles, ranks, etc, we'll need it later
games <- read_csv("../notes_and_info/0-gameinfo.csv", show_col_types = FALSE)
```


## Downsampling Data

In the data as I gathered it, positively reviewed games were quite over represented in the data. Because many of the positively reviewed games in the data are widely beloved games, they naturally have only gathered more and more reviews with time, with a majority of those being positive reviews. 

The number of reviews (total and in English) at [this table](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/notes_and_info/0-gameinfo.csv) were added manually by me using the total listing on the Steam App. Those numbers are accurate as of the date I downloaded my reviews (shown in my [projnotes.md](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/notes_and_info/projnotes.md)). However the script I used to download the reviews would time out after a certain point, and not every single review was downloaded. 

The original numbers are shown here.


```{r}
#| label: full-revs-total
full_df |>
  group_by(steam_id) |>
  summarise(total = n()) |>
  arrange(desc(total))

full_df |>
  group_by(review_type) |>
  summarise(total = n())
```


My theory here just comes down to popularity. As a game releases and players try it out and review it, it begins to migrate through the ranks of either positive or negative. As new players find out about the game, if it shows already that it's being negatively reviewed, why would they spend money to try it out themselves? I wouldn't! On the other hand, if players see that a game is getting positive reviews, they're more likely try it and, in turn, also positively review it.

The negatively reviewed games were very likely largely forgotten to time once they started the descent into the negative review rankings. After an initial flood of bad reviews, feedback fell off. 

So, there it is. Positively ranked games have overall more reviews than negatively ranked games, and there are more positive reviews than negative ones in my data as a result. 


Because of this, we decided the best fit would be a stratified downsampling of the total positive reviews. Positive reviews to be randomly, but proportionally, cut down in total to be made more equal to negatively reviewed games and as a result, more appropriately comparable. 


```{r}
#| label: revs-split
posreviews <-
  full_df |>
  filter(review_type=="POS")

negreviews<-
  full_df |>
  filter(review_type=="NEG")
```



```{r}
#| label: downsampling
# seed so randomly selected downsampled reviews will be the same
set.seed(1234)

# reviews to be analyzed, sampled proportionally by difference in neg & pos reviews per game
reviews_df <- posreviews |>
  slice_sample(prop=nrow(negreviews)/nrow(posreviews), by=steam_id)

reviews_df <- bind_rows(reviews_df, negreviews)

# a new df, much more manageable in size
reviews_df
```


```{r}
#| label: merged-games

#combine the reviews df and the game info df for all info needed
merged_games <- reviews_df |>
  left_join(games) 
```



## TO DO


2. similar toks charts not working why?????


## Getting a Look at the Data


```{r}
#| label: merged2
merged_games2 <- full_df |>
  left_join(games) 
```

```{r}
#| label: rev2
revcount_df2 <- merged_games2 |>
  group_by(game) |>
  summarise(total = n()) |>
  arrange(desc(total))
revcount_df2
```



### Reviews per game
 
```{r}
#| label: game-revs-c2
revcount_df <- merged_games |>
  group_by(game) |>
  summarise(total = n()) |>
  arrange(desc(total))
revcount_df
```

```{r}
#| label: split-data
cut1 <- revcount_df |>
  slice(1:10) |>
  #reorder by total so that the plot legend is in the right order
  mutate(game = fct_reorder(game, total, .desc = TRUE))

cut2 <- revcount_df |>
  slice(11:20) |>
  mutate(game = fct_reorder(game, total, .desc = TRUE))

cut3 <- revcount_df |>
  slice(21:29) |>
  mutate(game = fct_reorder(game, total, .desc = TRUE))

cut4 <- revcount_df |>
  slice(30:36)|>
  mutate(game = fct_reorder(game, total, .desc = TRUE))

cut5 <- revcount_df |>
  slice(37:45) |>
  mutate(game = fct_reorder(game, total, .desc = TRUE))
```


```{r}
#| label: revcount-bars
ggplot(cut1, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")

ggplot(cut2, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")

ggplot(cut3, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")

ggplot(cut4, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")

ggplot(cut5, aes(x = reorder(game, -total), y=total, fill=game)) +
  geom_bar(stat='identity', color="black") + 
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_blank()) +
  labs(x="Game", y = "No. of Reviews")
```

Take care to note the y-axis and how it changes between plots. The games with the fewest reviews are significantly less than the ones with the most. This is because of how the "Positive" and "Negative" categories are calculated, the cutoff point at those ranks is 50 reviews. Any more than that and the game will migrate to a different rank. 

Also please note that the numbers indicated here are not reflective of the actual number of reviews as shown on Steam (documented [here](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/notes_and_info/0-gameinfo.csv)), and only reflect the total number of reviews in *this* data set, which has been adjusted above for the sake of comparison.

Finally, I've mentioned before that it is the most positively reviewed games that had the most total reviews total, and that is *mostly* true. 


Just to get a full scope of the scale (and why I split the data up like I did above) here are all the totals in comparison.


```{r}
ggplot(revcount_df, aes(x = reorder(game, -total), y=total)) +
  geom_bar(stat='identity') +
  theme(axis.text.x = element_blank())
```

### Positive and Negative

How many reviews of each type are there?

```{r}
#| label: plot-reviews
ggplot(reviews_df, aes(x=review_type, fill=review_type )) + 
  geom_bar( width = 0.5) +
  scale_fill_brewer(palette = "Set2") +
  geom_text(stat = "count", aes(label = after_stat(count)), size = 3.5, vjust = 3, hjust = 0.5, position = "stack") +
  theme(legend.position="right")
```




Let's look at the actual count of reviews per game ranking, simplified down to only positive, negative, and mixed for ease.

```{r}
#| label: rank-rev-count
merged_games |>
  group_by(rank) |>
  #merging all varieties of pos/neg together by removing the first word per rank
  #very positive and overwhelmingly positive both just become positive etc
  mutate(rank = str_replace_all(rank, "\\w+ (\\w)", "\\1")) |>
  summarise(count = n())

```


It looks like yes! 




## Word Count

```{r}
#| label: word-count
reviews_df <- reviews_df |>
  mutate(word_count = str_count(tokens, '\\,')+1)

reviews_df
```



### Review length

With word counts per review added, let's take a look!

```{r}
#| label: WC-stats
summary(reviews_df$word_count)
```

So the average review is around 51/52 words long, and the median 17 words. 

TO DO: (maybe look into something about average text lengths online like for blogs etc)

Let's look a little deeper. 


```{r}
#| label: WC-low-high
reviews_df |>
  filter(word_count==1)

reviews_df |>
  filter(word_count>1700)
```

There's quite a few one-word reviews! Some of them are pretty reasonable... "Amazing", "Spooky", "Boring", "Unplayable". Short and sweet, gets the point across well enough. Others are a little less obvious. I saw a number of keyboard smash reviews, strings of numbers, I saw one that was just a rabbit emoji. The last one might be a reference I just don't understand.

Looking next at these longest reviews, I was really surprised! A huge majority of these are exactly the same phrase repeated over and over, and those repeated reviews are all for the same game, as well. I have to do some investigation... 

Checking my game info [here](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/notes_and_info/0-gameinfo.csv), I was able to confirm that these repeating reviews are all from "The Stanley Parable: Ultra Deluxe". 

Looking further, I found some more information. ["The end is never"](https://thestanleyparable.fandom.com/wiki/The_End_Is_Never...) is a tagline for the game itself, and appears within the game multiple times as a reference to the inescapable time loop the game's protagonist is stuck in. The phrase also apparently appears on the game's loading screens in a constant loop. 



### Average Review Length

Now to what we came here to look into. Are positive reviews typically longer, or negative reviews? 


```{r}
#| label: plot-avg-len
ggplot(reviews_df, aes(x=review_type, y = word_count)) + 
  stat_summary(fun = mean, geom = "bar", fill = "skyblue") +
  geom_text(aes(label = after_stat(sprintf("%.2f", y))), stat = "summary", fun = "mean", vjust = 3, hjust=0.5) +
  ylab("Avg Amount") +
  theme(legend.position="right")
```

Now that's really interesting! Despite having fewer reviews total, and *all* of the longest reviews we looked at above being positive, negative reviews are still quite a bit longer on average. People must have a lot more to say when they dislike a game than when they like one!

I wonder if there's a trend in shorter reviews that might be swinging this average one way or the other. 

```{r}
#| label: rev-type-count
reviews_df |>
  group_by(review_type) |>
  filter(word_count<6) |>
  summarise(word_count = sum(word_count))
```

Looks like it's pretty common for positive reviews to be shorter. This could potentially drag down the overall average of positive reviews. However... there are more short positive reviews, but there are also more positive reviews in general, so maybe not.

TO DO - fix this once the downsampling is done... 38.7% of all negative reviews are 5 words or shorter, and 74.4% of all positive reviews are. That's quite a big chunk of reviews! 


NOW look at average review length per game, add that together to look at per game rank too. 


### Length Stats

Word length histograms.... A LOT of reviews are all grouped together at the lower end of the spectrum
DO A LOG TRANSFORMATION TO THEM AND REDO

```{r}
#| label: pos-neg-revs
posrevs <- reviews_df |>
  filter(review_type=='POS')

negrevs <- reviews_df |>
  filter(review_type=='NEG')
```


```{r}
#| label: pos-neg-dist
pos <- ggplot(posrevs, aes(x=word_count)) + 
  geom_histogram(binwidth = 50) +
  scale_y_log10()
pos

neg <- ggplot(negrevs, aes(x=word_count)) + 
  geom_histogram(binwidth = 50) +
  scale_y_log10()
neg
```

Can I log transform this and then make box or violin plots? 





```{r}
#| label: WC-bplot
ggplot(reviews_df, aes(x=review_type, y=word_count)) + 
  geom_boxplot() +
  scale_y_log10()
  #geom_jitter(shape=16, position=position_jitter(0.2))
```

```{r}
#| label: WC-vplot
ggplot(reviews_df, aes(x=review_type, y=word_count)) + 
  geom_violin() +
  scale_y_log10()
```



## Word Types & Count

```{r}
#| label: GetTypes
remove_dupes <- function(x) {
  #split tokens to be iterated over
  words <- strsplit(x, " ")[[1]]
  #apply the unique() function per row
  unique_words <- unique(words)
  #re-collapse into rows
  paste(unique_words, collapse = " ")
}
```


```{r}
#| label: types-col
# get word types by removing duplicates from tokens rows
reviews_df$types <- sapply(reviews_df$tokens, remove_dupes)
```

```{r}
#| label: types-count
reviews_df <- reviews_df |>
  mutate(type_count = str_count(types, '\\,')+1)
```


## TTR 

TTR is used to measure the variety of language used in a text. It's not  exactly a measure of the complexity of a document, but can be considered with other factors as an indicator of a writer's language abilities. 

TTR is measured by dividing the total number of word types (unique words used) by total number of word tokens (all words used). A low score (closer to 0) indicates a highly repetitive document, and a high score (closer to 1) indicates a higher variety or words. A score of 1 would mean that no words were repeated in the document. 

TTR is very sensitive to document length. Too long, and documents taper off. There are only so many content words to be used in a document, eventually the highly repetitive function words will outnumber them. 

[source](https://medium.com/@rajeswaridepala/empirical-laws-ttr-cc9f826d304d)

```{r}
#| label: TTR
reviews_df <- reviews_df |>
  mutate(TTR = type_count/word_count)

reviews_df
```

### TTR Exploring 

idk think about what to put in here - probably just a general summary looking at pos/neg, it will likely mostly be aligned with review length over all, since so many reviews are so short.



## Tf-idf

Calculating Tf-idf for our game reviews. Because they were creating a lot of issues with tokenizing and odd characters, I've excluded emojis and gone simply for text reviews. It's a bummer to lose out on such strong reviews as "one single poop emoji", but needs to be done for ease of processing. For now, at least.

```{r}
#| label: rm-stopwords
data(stop_words)

tokens_df <- reviews_df |>
  unnest_tokens(word, review) |>
  anti_join(stop_words)

tokens_df <- tokens_df |>
  count(word, sort = TRUE) 

tokens_df
```


By tokenizing and removing stop words, which in this case also removed all emojis and special characters, we went from around 11 million tokens (from [data cleanup tokenizing](https://github.com/Data-Sci-2025/Steam-Reviews-Analysis/blob/main/data_processing/1-data-cleanup.md#tokenizing)) down to 3.6 million (from calculating sum(n) on tokens_df).

The most common word token among these game reviews.... is game! 

Looking through the first few pages of tokens, I wonder if we can pick out some key words of game elements reviewers find important enough to comment on specifically. I see story, gameplay, characters, hours (game length, probably), worth (game price, I bet), puzzles, pretty, money, music, graphics... all in the top 50 words. 

What about top words per review type? 

REDO THIS merge into reviews_df as a new df and do the pos/neg thing by filtering instead. 


```{r}
#| label: pos-topwords
pos_toks <- posrevs |>
  unnest_tokens(word, review) |>
  anti_join(stop_words) |>
  count(word, sort = TRUE)
pos_toks
```

```{r}
#| label: neg-topwords
neg_toks <- negrevs |>
  unnest_tokens(word, review) |>
  anti_join(stop_words) |>
  count(word, sort = TRUE)
neg_toks
```



### TF

calculate the frequency for each word for the works of Jane Austen, the BrontÃ« sisters, and H.G. Wells by binding the data frames together - RECALCULATE THIS TOO - per review, per review type, recheck activity 9 to make sure that seems right


```{r}
#| label: rev-freq
frequency <- tokens_df |>
  mutate(proportion = n / sum(n)) |>
  select(-n)

frequency
```

This means that 64% of all the words in game reviews is the word game???


### Plotting

```{r}
#| label: pos-neg-freq
freq <- bind_rows(mutate(pos_toks, review_type = "Positive"),
                       mutate(neg_toks, review_type = "Negative")) |> 
  mutate(word = str_extract(word, "[a-z']+")) |>
  count(review_type, word) |>
  group_by(review_type) |>
  mutate(proportion = n / sum(n)) |> 
  select(-n) |> 
  pivot_wider(names_from = review_type, values_from = proportion) |>
  pivot_longer(Positive:Negative,
               names_to = "review_type", values_to = "proportion")

freq
```


### In progress: similar tokens pos & neg

```{r}
#| label: similar-toks
#| eval: false
ggplot(freq, aes(x = proportion, y = review_type, 
                      color = abs(proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~review_type, ncol = 2) +
  theme(legend.position="none") 
```

+
  labs(y = "Jane Austen", x = NULL)


Words closest to the line have similar frequencies in both samples

Also:
not all the words are found in all three sets of texts and there are fewer data points in the panel for Austen and H.G. Wells.

quantify how similar and different these sets of word frequencies are using a correlation test


### term freq


```{r}
#| label: revtoks-totals
review_words <- reviews_df |>
  unnest_tokens(word, review) |>
  anti_join(stop_words) |>
  count(review_id, word, sort = TRUE)

total_words <- review_words |> 
  group_by(review_id) |> 
  summarize(total = sum(n))

review_words <- left_join(review_words, total_words)

review_words <- left_join(reviews_df, review_words)

review_words <- review_words |>
  select(review_id, review_type, word, n, total)

review_words
```

Calculating the total word counts per word and by review type and comparing to the total word counts by review type. With these, we can get moving on tf-idf.


```{r}
#| label: revtoks-freq
ggplot(review_words, aes(n/total, fill = review_type)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~review_type, ncol = 2, scales = "free_y")
```

### bind_tf_idf() function

find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents

- which words are the words that define the text?

- what words are common (but not too common)?

bind_tf_idf()

takes a tidy text dataset as input with one row per token (term), per document

  - one column contains the terms/tokens (word)
  - one column contains the documents (review type)
  - last necessary column contains the counts, how many times each document contains each term (n)
  
  maybe edit to carry in also steam_id I think, just so it's in there.
  MAYBE do this from the merged games df - but you have to re-merge it because the original is actually merged before the word counts etc have been added in. THEN save the game names and also the rank - simplify that rank in a new column to "pos, mixed, neg" and try that with the classifier. If we can calculate % of pos/neg ratio of reviews per game can we predict the game's rank? 
  
```{r}
#| label: review-tfidf
review_tf_idf <- review_words |>
  bind_tf_idf(word, review_id, n)

review_tf_idf
```

with stop words included there are 6,412,302 total rows
with stop words excluded it went down to 3,045,610!

```{r}
#| label: high-count-words
review_tf_idf |>
  arrange(desc(n))
```


```{r}
#| label: tf-df-final
review_tf_idf <- review_tf_idf |>
  select(review_id, review_type, word, total, tf_idf)

review_tf_idf
```







```{r}
#| label: write_tf-idf-csv
write_csv(review_tf_idf, file="../private/reviews_tfidf.csv")
```



```{r}
#| label: jim-review
reviews_df |>
  filter(review_id==161023893)
```









how to quantify what a document is about
 
tf - term frequency - how frequently a word shows up
idf - inverse document frequency- how many documents a word appears in (no of docs / no of dccs containing term)

tf-idf: frequency of a term adjusted for how rarely it is used

- intended to measure how important a word is to a document in a collection (or corpus) of documents




Some notes to pay attention to:

idf and thus tf-idf are zero for these extremely common words
  - words that appear in all six of austen's novels
  - this decreases the weight for these extremely common words
  
idf will be a higher number for words that occur in fewer of the documents in the collection

To take a look at high idf terms:

```{r}
#| label: rev-high-tfidf
review_tf_idf %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

SUMMARY - these are informative in the sense that they are not likley to be replicated, I suppose. Not so much in like. discerning meaning.  

```{r}
#| label: aust-propnouns
#| eval: false
book_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

We can conclude: jane austen uses a lot of the same language between her books, and the thing that really distinguishes them is the characters in them and locations.



